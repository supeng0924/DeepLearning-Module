# 轻量化网络

## 1. MobileNet

1) MobileNet V1：引入深度可分离卷积  

将标准的3x3卷积替换为3x3DW+1x1PW  

参数量由原来的 $D_kD_kMND_FD_F$ 变为 $D_kD_KMD_FD_F+MND_FD_F$ , 减少至$\frac{1}{8}～\frac{1}{9}$   

同时有两个参数 $\begin{cases}width因子:通道数x\alpha \\分辨率因子:分辨率减少\rho^2 \end{cases}$   

2) MobileNet V2  
$$
提出了两点 \begin{cases}反向残差:先1x1卷积升维，再3x3DW卷积，最后1x1卷积降维 \\线性瓶颈:最后激活函数采用线性函数，非ReLU \end{cases}
$$

```
为什么采用线性瓶颈而非ReLU？
为了保证模型的表达能力。降维之后，有信息损失掉，再用ReLU会有较大的信息损失。因为为了减少信息的损失采用了线性瓶
颈。文中关于ReLU对不同的输入维度信息做了实验，维度增加15或者30后，再ReLU不会损失太多，反而增加2或3损失信息较
多。Mobilenet V2中降维后维度等于输入维度，用ReLU会损失较多信息。

ReLU6：限制最大输出为6
为了在移动端设备float16的低精度也能有很好的分辨率，如果分布范围太大，低精度float16无法精确描述
```

3) MobileNet V3  

互补搜索技术：由资源受限的NAS执行模块级搜索，NetAdapt执行局部搜索  

网络结构改进：将最后一步的平均池化前移并移除最后一个卷积层，引入h-swish激活函数 $h-swish(x)=x*\frac{ReLU6(x+3)}{6}$  

## SqueezeNet

```
采用不同的卷积方式，提出fire module，包括两部分 squeeze层和expand层
squeeze层：1x1卷积(卷积核少于上层)
expand层：1x1卷积+3x3卷积，然后concat
```

## ShuffleNet

```
1. shuffleNet v1： 分组进行PW卷积，同时进行通道洗牌   
2. shuffleNet v2：提出模型设计准则，并基于准则改进网络 
		1) 通道分成2组
		2) 分割之后，左侧直接映射，右侧Cin和Cout一样的深度可分离卷积
		3) 右侧并没有分组卷积，而是先1x1->3x3DW->1X1
		4) 合并采用concat
```



## 轻量化网络的研究方向

```
1.人工设计轻量化网络
2.基于神经网络架构搜索的自动化设计神经网络
3.CNN压缩
4.基于AutoML的自动模型压缩
```

