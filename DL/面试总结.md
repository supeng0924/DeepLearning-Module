# Deep Learning summary
[toc]



### 1. 卷积

```
1.概念：卷积核和感受野中的元素相乘后求和的结果，并滑动完整个图像的过程
2.作用：
	1) 提取图像的特征，并且卷积核权重可学习，能突破传统滤波器的限制，根据目标函数学习出想要的特征
	2) “局部感知，参数共享”，大大降低了参数量，保证网络的稀疏性，防止过拟合
3.小卷积核相对于大卷积核的优势：
	1) 增加了网络容量和模型复杂度：小卷积核的多层叠加，加深了网络的深度
	2) 减少了参数量，例如
4.1x1卷积核作用
	1) 可以升维和降维
	2) 可以实现跨通道信息交互和融合
	3) 以较少的参数量加宽加深网络的层数，加入更多的非线性
```

### 2. 池化

```
1.概念：将图像缩小，减少像素信息，只保留重要的信息，只要为了减少参数量
2.作用：
	1) 特征不变性：汇合操作更关注某些特征而不是具体位置，所以能容忍一些微小的偏移
	2) 特征降维：汇合结果中的一个元素对应原始输入数据的一个子区域，相当于在范围空间做了维度的约减
	3) 一定程度上可以防止过拟合
```

### 3. BN

```
1.深度神经网络难训练的原因：
	1) 层与层之间存在高度关联性和耦合性，会导致底层参数微弱变化，由于每层的线性变换与非线性激活，随着网络层数加深而被放大。
	2) 参数的变化导致每一层的输入分布会发生改变，进而上层的网络需要不停地去适应这些分布的变化，使得模型训练变得困难，这一现象叫internal covariate shift

2.BN内部原理： BN通过一定的规范化手段，对于每一个隐层神经元把逐渐向非线性饱和区靠拢的输入强制拉回到均值为0，方差为1的标准正态分布，使得非线性输入落入对输入比较敏感的区域，以此避免梯度消失，同时为了防止网络表达能力下降，引入缩放和平移参数，通过训练来学习，使得表达能力增强。

3.好处：
	1) 调参过程变简单，对初始化要去没那么高，可以使用大学习率
	2) 降低了数据间的绝对差异，更多的考虑相对差异性
	3) 本身是一种正则化
```

BN的计算过程：

1.对mini-batch 求均值   $\mu_{B}=\frac{1}{m}\sum_{i=1}^mx_i$        

2.对mini-batch求方差 $\sigma_B^2=\frac{1}{m}\sum_{i=1}^m(x_i-\mu_B) $   

3.标准化数据 $\widehat{x_i}=\frac{(x_i-\mu_B)^2}{\sqrt{\sigma_B^2+\epsilon}}$   

4.缩放和平移 $y=\nu\widehat{x_i}+\beta$  

```
BN(batch normalization)是在batch上对N,H,W三个维度做归一化，保留C的维度。适用于固定深度的前向神经网络，如CNN
LN(layer normalization)是在通道上对C，H，W做归一化
IN(instance normalization)对H，W做归一化
GN(group normalization)，对每个group(C/G),H,W做归一化，适用于占显存较大的任务
```

BN中如果batch特别小的话，效果不好。如果batch大的话，反向传播梯度更稳定

### 4. 梯度消失和梯度爆炸

```
1.梯度爆炸：反向传播对激活函数求导，如果导数大于1，随着网络层数的增加，梯度朝着指数爆炸的形式增加
2.梯度消失:---------------------------小于1，------------------------衰减的方式减少
3.解决方案：
	1) 使用预训练网络进行微调
	2) 使用残差结构
	3) 梯度剪切：针对梯度爆炸
	4) 激活函数使用relu，leak relu等
	5) BN
```

### 5. 过拟合和欠拟合

```
过拟合：对于训练好的的模型，若在训练集上表现好，却在测试集上表现差，这便是过拟合
欠拟合：---------------，-------------差，不必说在测试集上表现同样会差，这便是欠拟合
欠拟合解决方法：
	1) 继续训练
	2) 增加新特征
	3) 尝试非线性
	4) 如果有正则项，减小正则小参数
	5) boosting
过拟合解决方法：
	1) 数据增强
	2) 特征选择，减少特征
	3) BN
	4) 正则化如果有，增大正则项参数
	5) 交叉检验
	6) 早停(early stop)
	7) bagging
```

### 6. 正则化

L1：$\sum_i^{}|w|$     $\frac{\partial L}{\partial w_i}=sign(w_i)=\begin{cases}+1 & w_i > 0\\-1 & w_i <= 0\end{cases}$   

L2：$\frac{1}{2}\sum_i^{}w_i^2$    $\frac{\partial L}{\partial w_i}=w_i$   

L1更新公式   $w_i=w_i-\eta*1$  

L2更新公式   $w_i=w_i-\eta*w_i$  

```
正则化作用：保证模型尽可能简单，避免过拟合
L1特点：能产生等于0的值权值，剔除某些特征，模型更容易解释
L2特点：可以迅速得到比较小的权值，却难以收敛到0，产生平滑的效果，计算效率高，有解析解
L1正则是拉普拉斯先验，L2正则是高斯先验
```

### 7. 样本不均衡

```
不均衡：每个类别下的样本数目相差很大
解决方法：
	1) 扩大数据集
	2) 数据集重采样:小样本过采样，大类样本欠采样
	3) 对模型进行惩罚，增加小类样本权重，降低大类样本的权重
	4) 决策树
```

### 8. 卷积为什么高效

```
Yann Lecun表明对于图像数据，数据的信息与结构在语义层面上都是组合性的，整体图像的语义是由局部组合而成，深度网络这种层级表征结构能依次从简单特征组合成复杂的抽象特征。
```

### 9. 全卷积网络

```
定义：没有全连接层
优点：
	1) 支持不同大小的输入，
	2) 支持端到端的训练
	3) 适合输出是图像的任务，如分割，边缘检测，光流
```

输出特征图大小计算：  $n_{out}=\lfloor\frac{n_{in}+2p-k}{s}\rfloor$   

感受野大小计算：        $l_k=l_{k-1}+[(f_k-1)*\prod_{i=1}^{k-1}s_i]$  

### 10. BP算法

BP算法全称是Back Propogation Algorithm，是一种更新权重的方法，在BP算法中，权重的更新是这样的
$$
W_l=W_l-\eta\frac{\partial{C}}{\partial{W_l}}　　其中C是损失函数，　\eta是学习率
$$
通过链式法则求出每一层的梯度，然后对每一层权值进行更新。



